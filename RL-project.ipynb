{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  *Develop an agent that can achieve a specific goal using a sub-goal in order to accelarate learnign in a  maze 10X10 based on RL algorithm (Q-learning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries to create the maze and give our agent the ability to randomly making choices\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setting up the environment WITH SUB-GOAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemetation of the maze 10X10 as indicated in the assignment. \n",
    "\n",
    "# 1 represents the walls (unpassable)\n",
    "# 0 represents an open path (passable)\n",
    "# S marks the starting point of the agent\n",
    "# SG marks the sub-goal the agent should reach before continuing\n",
    "# End marks the end goal\n",
    "\n",
    "maze = np.array([[1,1,1,1,1,1,1,1,1,1],\n",
    "                 [1,'S',1,0,0,0,1,0,0,1],\n",
    "                 [1,0,1,0,1,0,1,0,1,1],\n",
    "                 [1,0,0,0,1,0,0,0,0,1],\n",
    "                 [1,1,1,0,1,1,1,1,0,1],\n",
    "                 [1,0,1,'SG',0,0,0,1,0,1],\n",
    "                 [1,0,0,0,1,1,0,0,0,1],\n",
    "                 [1,1,1,0,1,0,1,1,0,1],\n",
    "                 [1,0,0,0,0,0,1,'End',0,1],\n",
    "                 [1,1,1,1,1,1,1,1,1,1]], dtype=object)\n",
    "\n",
    "\n",
    "\n",
    "# Implementatio of the Q-Table\n",
    "# Considering the maze above, and the Q-learning algorithm, we need to initiate\n",
    "# the Q-table. The Q-table include all possible states-actions pairs and their corresponing Q-values.\n",
    "\n",
    "q_table = np.zeros((10,10,4))   # each states have 4 possible actions (up, down, left, right)\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Implementation of the agent's starting position in the maze, the sub-goal and the end-goal\n",
    "\n",
    "start_position = (1,1)\n",
    "sub_goal = (5,3)\n",
    "end_goal = (8,7)\n",
    "\n",
    "\n",
    "# Implementation of the reward system\n",
    "\n",
    "reward_sub_goal = 10\n",
    "reward_end_goal = 100\n",
    "penalty = -1    \n",
    "\n",
    "# Implementation of the agent's interactions with the environment, the way the agent moves in the maze\n",
    "# and the way penalties and rewards are given to the agent.\n",
    "\n",
    "def moves_subgoal(state, actions):\n",
    "    row, col = state\n",
    "\n",
    "    if actions == 'up':\n",
    "        next_state = (row - 1, col)\n",
    "    elif actions == 'down':\n",
    "        next_state = (row + 1, col)\n",
    "    elif actions == 'left':\n",
    "        next_state = (row, col - 1)\n",
    "    elif actions == 'right':\n",
    "        next_state = (row, col + 1)\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "# Check if the next state is a wall or out of the maze\n",
    "    if 0 <= next_state[0] < 10 and 0 <= next_state[1] < 10:\n",
    "        if maze[next_state] != 1:\n",
    "            if next_state == sub_goal:\n",
    "                return next_state, reward_sub_goal, False   #Sub-goal reached \n",
    "            elif next_state == end_goal:\n",
    "                return next_state, reward_end_goal, True    #End-goal reached\n",
    "            else:\n",
    "                return next_state, penalty, False           #No goal reached, keep searching\n",
    "        else:\n",
    "            return state, penalty, False                    #Hit a wall, stay in the same state\n",
    "    else:\n",
    "        return state, penalty, False                        #Out of the maze, stay in the same state\n",
    "\n",
    "\n",
    "\n",
    "# Implementation of the starting funtion, in order to restart the agent's position in the maze\n",
    "# at the end of each episode.\n",
    "\n",
    "def restarting():\n",
    "    return start_position\n",
    "\n",
    "\n",
    "# Implementation of the selection of the agent's action.\n",
    "# The agent will randomly choose the actions to take in the Q-table, based on the epsilon-greedy policy.\n",
    "# The action with the highest Q-value will be chosen \n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return np.argmax(q_table[state[0], state[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setting up the environment WITHOUT SUB-GOAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemetation of the maze 10X10 as indicated in the assignment. \n",
    "\n",
    "# 1 represents the walls (unpassable)\n",
    "# 0 represents an open path (passable)\n",
    "# S marks the starting point of the agent\n",
    "# End marks the end goal\n",
    "\n",
    "maze = np.array([[1,1,1,1,1,1,1,1,1,1],\n",
    "                 [1,'S',1,0,0,0,1,0,0,1],\n",
    "                 [1,0,1,0,1,0,1,0,1,1],\n",
    "                 [1,0,0,0,1,0,0,0,0,1],\n",
    "                 [1,1,1,0,1,1,1,1,0,1],\n",
    "                 [1,0,1,0,0,0,0,1,0,1],\n",
    "                 [1,0,0,0,1,1,0,0,0,1],\n",
    "                 [1,1,1,0,1,0,1,1,0,1],\n",
    "                 [1,0,0,0,0,0,1,'End',0,1],\n",
    "                 [1,1,1,1,1,1,1,1,1,1]], dtype=object)\n",
    "\n",
    "\n",
    "\n",
    "# Implementatio of the Q-Table\n",
    "# Considering the maze above, and the Q-learning algorithm, we need to initiate\n",
    "# the Q-table. The Q-table include all possible states-actions pairs and their corresponing Q-values.\n",
    "\n",
    "q_table = np.zeros((10,10,4))   # each states have 4 possible actions (up, down, left, right)\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Implementation of the agent's starting position in the maze, the sub-goal and the end-goal\n",
    "\n",
    "start_position = (1,1)\n",
    "end_goal = (8,7)\n",
    "\n",
    "\n",
    "# Implementation of the reward system\n",
    "\n",
    "reward_end_goal = 100\n",
    "penalty = -1    \n",
    "\n",
    "# Implementation of the agent's interactions with the environment, the way the agent moves in the maze\n",
    "# and the way penalties and rewards are given to the agent.\n",
    "\n",
    "def moves_NO_subgoal(state, actions):\n",
    "    row, col = state\n",
    "\n",
    "    if actions == 'up':\n",
    "        next_state = (row - 1, col)\n",
    "    elif actions == 'down':\n",
    "        next_state = (row + 1, col)\n",
    "    elif actions == 'left':\n",
    "        next_state = (row, col - 1)\n",
    "    elif actions == 'right':\n",
    "        next_state = (row, col + 1)\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "# Check if the next state is a wall or out of the maze\n",
    "    if 0 <= next_state[0] < 10 and 0 <= next_state[1] < 10:\n",
    "        if maze[next_state] != 1:\n",
    "            if next_state == end_goal:\n",
    "                return next_state, reward_end_goal, True    #End-goal reached\n",
    "            else:\n",
    "                return next_state, penalty, False           #No goal reached, keep searching\n",
    "        else:\n",
    "            return state, penalty, False                    #Hit a wall, stay in the same state\n",
    "    else:\n",
    "        return state, penalty, False                        #Out of the maze, stay in the same state\n",
    "\n",
    "\n",
    "\n",
    "# Implementation of the starting funtion, in order to restart the agent's position in the maze\n",
    "# at the end of each episode.\n",
    "\n",
    "def restarting():\n",
    "    return start_position\n",
    "\n",
    "\n",
    "# Implementation of the selection of the agent's action.\n",
    "# The agent will randomly choose the actions to take in the Q-table, based on the epsilon-greedy policy.\n",
    "# The action with the highest Q-value will be chosen \n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return np.argmax(q_table[state[0], state[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.1  # Discount factor\n",
    "epsilon = 1.0  # Initial exploration rate. \n",
    "epsilon_variation = 0.99  # This parameter will decrease the epsilon value over time, in order to make the agent more greedy\n",
    "min_epsilon = 0.01  # Minimum exploration rate reached by the agent\n",
    "episodes = 10000 # Number of episodes the agent will run\n",
    "steps = 100  # Number of steps the agent will take in each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Q-learning algorithm based on the mathematical formula\n",
    "# Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))\n",
    "\n",
    "def q_value(state, action_index, reward, next_state):\n",
    "    \n",
    "    max_q_value = np.max(q_table[next_state[0], next_state[1]])\n",
    "    actual_q_value = q_table[state[0], state[1], action_index]\n",
    "    new_q_value = actual_q_value + alpha * (reward + gamma * max_q_value - actual_q_value)\n",
    "\n",
    "    q_table[state[0], state[1], action_index] = new_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/10000, Total reward: 48\n",
      "Episode 200/10000, Total reward: 373\n",
      "Episode 300/10000, Total reward: 395\n",
      "Episode 400/10000, Total reward: 428\n",
      "Episode 500/10000, Total reward: 428\n",
      "Episode 600/10000, Total reward: 428\n",
      "Episode 700/10000, Total reward: 406\n",
      "Episode 800/10000, Total reward: 406\n",
      "Episode 900/10000, Total reward: 428\n",
      "Episode 1000/10000, Total reward: 428\n",
      "Episode 1100/10000, Total reward: 428\n",
      "Episode 1200/10000, Total reward: 428\n",
      "Episode 1300/10000, Total reward: 428\n",
      "Episode 1400/10000, Total reward: 428\n",
      "Episode 1500/10000, Total reward: 417\n",
      "Episode 1600/10000, Total reward: 428\n",
      "Episode 1700/10000, Total reward: 417\n",
      "Episode 1800/10000, Total reward: 428\n",
      "Episode 1900/10000, Total reward: 417\n",
      "Episode 2000/10000, Total reward: 428\n",
      "Episode 2100/10000, Total reward: 428\n",
      "Episode 2200/10000, Total reward: 428\n",
      "Episode 2300/10000, Total reward: 417\n",
      "Episode 2400/10000, Total reward: 406\n",
      "Episode 2500/10000, Total reward: 428\n",
      "Episode 2600/10000, Total reward: 428\n",
      "Episode 2700/10000, Total reward: 428\n",
      "Episode 2800/10000, Total reward: 428\n",
      "Episode 2900/10000, Total reward: 428\n",
      "Episode 3000/10000, Total reward: 428\n",
      "Episode 3100/10000, Total reward: 428\n",
      "Episode 3200/10000, Total reward: 428\n",
      "Episode 3300/10000, Total reward: 417\n",
      "Episode 3400/10000, Total reward: 428\n",
      "Episode 3500/10000, Total reward: 417\n",
      "Episode 3600/10000, Total reward: 428\n",
      "Episode 3700/10000, Total reward: 428\n",
      "Episode 3800/10000, Total reward: 428\n",
      "Episode 3900/10000, Total reward: 428\n",
      "Episode 4000/10000, Total reward: 428\n",
      "Episode 4100/10000, Total reward: 417\n",
      "Episode 4200/10000, Total reward: 417\n",
      "Episode 4300/10000, Total reward: 428\n",
      "Episode 4400/10000, Total reward: 428\n",
      "Episode 4500/10000, Total reward: 428\n",
      "Episode 4600/10000, Total reward: 428\n",
      "Episode 4700/10000, Total reward: 428\n",
      "Episode 4800/10000, Total reward: 428\n",
      "Episode 4900/10000, Total reward: 417\n",
      "Episode 5000/10000, Total reward: 428\n",
      "Episode 5100/10000, Total reward: 428\n",
      "Episode 5200/10000, Total reward: 417\n",
      "Episode 5300/10000, Total reward: 428\n",
      "Episode 5400/10000, Total reward: 428\n",
      "Episode 5500/10000, Total reward: 428\n",
      "Episode 5600/10000, Total reward: 428\n",
      "Episode 5700/10000, Total reward: 428\n",
      "Episode 5800/10000, Total reward: 428\n",
      "Episode 5900/10000, Total reward: 428\n",
      "Episode 6000/10000, Total reward: 417\n",
      "Episode 6100/10000, Total reward: 417\n",
      "Episode 6200/10000, Total reward: 428\n",
      "Episode 6300/10000, Total reward: 417\n",
      "Episode 6400/10000, Total reward: 417\n",
      "Episode 6500/10000, Total reward: 417\n",
      "Episode 6600/10000, Total reward: 417\n",
      "Episode 6700/10000, Total reward: 428\n",
      "Episode 6800/10000, Total reward: 428\n",
      "Episode 6900/10000, Total reward: 428\n",
      "Episode 7000/10000, Total reward: 428\n",
      "Episode 7100/10000, Total reward: 428\n",
      "Episode 7200/10000, Total reward: 428\n",
      "Episode 7300/10000, Total reward: 428\n",
      "Episode 7400/10000, Total reward: 417\n",
      "Episode 7500/10000, Total reward: 428\n",
      "Episode 7600/10000, Total reward: 417\n",
      "Episode 7700/10000, Total reward: 428\n",
      "Episode 7800/10000, Total reward: 428\n",
      "Episode 7900/10000, Total reward: 428\n",
      "Episode 8000/10000, Total reward: 417\n",
      "Episode 8100/10000, Total reward: 406\n",
      "Episode 8200/10000, Total reward: 428\n",
      "Episode 8300/10000, Total reward: 417\n",
      "Episode 8400/10000, Total reward: 417\n",
      "Episode 8500/10000, Total reward: 406\n",
      "Episode 8600/10000, Total reward: 428\n",
      "Episode 8700/10000, Total reward: 417\n",
      "Episode 8800/10000, Total reward: 417\n",
      "Episode 8900/10000, Total reward: 428\n",
      "Episode 9000/10000, Total reward: 417\n",
      "Episode 9100/10000, Total reward: 428\n",
      "Episode 9200/10000, Total reward: 417\n",
      "Episode 9300/10000, Total reward: 428\n",
      "Episode 9400/10000, Total reward: 417\n",
      "Episode 9500/10000, Total reward: 417\n",
      "Episode 9600/10000, Total reward: 428\n",
      "Episode 9700/10000, Total reward: 428\n",
      "Episode 9800/10000, Total reward: 428\n",
      "Episode 9900/10000, Total reward: 428\n",
      "Episode 10000/10000, Total reward: 417\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    state = restarting()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    #if episode < 5:  # Print debug information for the first few episodes\n",
    "    #    print(f\"\\nStarting Episode {episode + 1}\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        action_index = select_action(state, epsilon)\n",
    "        action = actions[action_index]\n",
    "\n",
    "        # Debug: Print selected action and state\n",
    "    #    if episode < 5:\n",
    "    #        print(f\"Step {step + 1}: State: {state}, Action: {action}\")\n",
    "\n",
    "        next_state, reward, done = moves_subgoal(state, action)\n",
    "\n",
    "        # Debug: Print next state, reward, and done flag\n",
    "    #    if episode < 5:\n",
    "    #        print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update Q-value\n",
    "        max_future_q = np.max(q_table[next_state[0], next_state[1]])\n",
    "        current_q = q_table[state[0], state[1], action_index]\n",
    "        q_table[state[0], state[1], action_index] = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "    #        if episode < 5:\n",
    "    #            print(f\"Episode {episode + 1} completed after {step + 1} steps with total reward {total_reward}\")\n",
    "           break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_variation)\n",
    "\n",
    "    # Print progress every 100 episodes\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f'Episode {episode + 1}/{episodes}, Total reward: {total_reward}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extras for checking the performance of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward per Episode: 4478.00\n",
      "Average Steps per Episode: 1000.00\n",
      "Sub-goal Reach Count: 49700\n",
      "         1600277 function calls in 4.550 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   100000    0.162    0.000    0.162    0.000 1712913629.py:45(moves)\n",
      "      100    0.000    0.000    0.000    0.000 3126056008.py:73(restarting)\n",
      "   100000    0.269    0.000    0.752    0.000 3126056008.py:81(select_action)\n",
      "        1    2.429    2.429    4.550    4.550 745427005.py:4(evaluate_policy)\n",
      "        1    0.000    0.000    4.550    4.550 <string>:1(<module>)\n",
      "        2    0.001    0.000    0.001    0.000 _tensor.py:929(__format__)\n",
      "   100000    0.018    0.000    0.018    0.000 fromnumeric.py:1136(_argmax_dispatcher)\n",
      "   100000    0.136    0.000    0.373    0.000 fromnumeric.py:1140(argmax)\n",
      "   100000    0.085    0.000    0.237    0.000 fromnumeric.py:53(_wrapfunc)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:203(schedule)\n",
      "        6    0.000    0.000    0.000    0.000 iostream.py:444(_is_master_process)\n",
      "        6    0.000    0.000    0.000    0.000 iostream.py:465(_schedule_flush)\n",
      "        6    0.000    0.000    0.000    0.000 iostream.py:535(write)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:90(_event_pipe)\n",
      "   100000    0.077    0.000    0.092    0.000 random.py:517(uniform)\n",
      "        1    0.000    0.000    0.000    0.000 socket.py:613(send)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1118(_wait_for_tstate_lock)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1185(is_alive)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:568(is_set)\n",
      "        1    0.000    0.000    4.550    4.550 {built-in method builtins.exec}\n",
      "   100000    0.024    0.000    0.024    0.000 {built-in method builtins.getattr}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "        2    0.010    0.005    0.010    0.005 {built-in method torch.mean}\n",
      "   100100    0.697    0.000    0.697    0.000 {built-in method torch.tensor}\n",
      "        2    0.015    0.007    0.015    0.007 {built-in method torch.zeros}\n",
      "        6    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__format__' of 'float' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "   100000    0.128    0.000    0.128    0.000 {method 'argmax' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   600002    0.484    0.000    0.484    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
      "   100000    0.014    0.000    0.014    0.000 {method 'random' of '_random.Random' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "# Define your evaluation function\n",
    "def evaluate_policy():\n",
    "    sub_goal_reach_count = 0\n",
    "    total_rewards = torch.zeros(100)  # Assuming you're not using CUDA for now\n",
    "    steps_taken = torch.zeros(100)\n",
    "\n",
    "    for episode in range(100):  # Set the number of evaluation episodes\n",
    "        state = torch.tensor(restarting())\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "\n",
    "        while not done and episode_steps < 1000:  # Limit steps per episode\n",
    "            action_index = select_action((state[0].item(), state[1].item()), 0)  # Always choose the best action\n",
    "            action = actions[action_index]\n",
    "            next_state, reward, done = moves((state[0].item(), state[1].item()), action)\n",
    "            next_state = torch.tensor(next_state)\n",
    "\n",
    "            if (state[0].item(), state[1].item()) == sub_goal:\n",
    "                sub_goal_reach_count += 1\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "        total_rewards[episode] = episode_reward\n",
    "        steps_taken[episode] = episode_steps\n",
    "\n",
    "    print(f\"Average Reward per Episode: {torch.mean(total_rewards):.2f}\")\n",
    "    print(f\"Average Steps per Episode: {torch.mean(steps_taken):.2f}\")\n",
    "    print(f\"Sub-goal Reach Count: {sub_goal_reach_count}\")\n",
    "\n",
    "# Run the profiler on the evaluation function\n",
    "cProfile.run('evaluate_policy()')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
