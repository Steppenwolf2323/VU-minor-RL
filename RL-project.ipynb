{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  *Develop an agent that can achieve a specific goal and sub-goal in a maze 10X10 based on RL algorithm (Q-learning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries to create the maze and give our agent the ability to randomly making choices\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemetation of the maze 10X10 as indicated in the assignment. \n",
    "\n",
    "# 1 represents the walls (unpassable)\n",
    "# 0 represents an open path (passable)\n",
    "# S marks the starting point of the agent\n",
    "# G marks the sub-goal the agent should reach before continuing\n",
    "# E marks the end goal\n",
    "\n",
    "maze = np.array([[1,1,1,1,1,1,1,1,1,1],\n",
    "                 [1,'S',1,0,0,0,1,0,0,1],\n",
    "                 [1,0,1,0,1,0,1,0,1,1],\n",
    "                 [1,0,0,0,1,0,0,0,0,1],\n",
    "                 [1,1,1,0,1,1,1,1,0,1],\n",
    "                 [1,0,1,'G',0,0,0,1,0,1],\n",
    "                 [1,0,0,0,1,1,0,0,0,1],\n",
    "                 [1,1,1,0,1,0,1,1,0,1],\n",
    "                 [1,0,0,0,0,0,1,'E',0,1],\n",
    "                 [1,1,1,1,1,1,1,1,1,1]], dtype=object)\n",
    "\n",
    "\n",
    "\n",
    "# Implementatio of the Q-Table\n",
    "# Considering the maze above, and the Q-learning algorithm, we need to initiate\n",
    "# the Q-table. The Q-table include all possible states-actions pairs and their corresponing Q-values.\n",
    "\n",
    "q_table = np.zeros((10,10,4))   # each states have 4 possible actions (up, down, left, right)\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Implementation of the agent's starting position in the maze, the sub-goal and the end-goal\n",
    "\n",
    "start_position = (1,1)\n",
    "sub_goal = (5,3)\n",
    "end_goal = (8,7)\n",
    "\n",
    "\n",
    "# Implementation of the reward system\n",
    "\n",
    "reward_sub_goal = 10\n",
    "reward_end_goal = 100\n",
    "penalty = -1    \n",
    "\n",
    "# Implementation of the agent's interactions with the environment, the way the agent moves in the maze\n",
    "# and the way penalties and rewards are given to the agent.\n",
    "\n",
    "def moves(state, actions):\n",
    "    row, col = state\n",
    "\n",
    "    if actions == 'up':\n",
    "        next_state = (row - 1, col)\n",
    "    elif actions == 'down':\n",
    "        next_state = (row + 1, col)\n",
    "    elif actions == 'left':\n",
    "        next_state = (row, col - 1)\n",
    "    elif actions == 'right':\n",
    "        next_state = (row, col + 1)\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "# Check if the next state is a wall or out of the maze\n",
    "    if 0 <= next_state[0] < 10 and 0 <= next_state[1] < 10:\n",
    "        if maze[next_state] != 1:\n",
    "            if next_state == sub_goal:\n",
    "                return next_state, reward_sub_goal, False   #Sub-goal reached \n",
    "            elif next_state == end_goal:\n",
    "                return next_state, reward_end_goal, True    #End-goal reached\n",
    "            else:\n",
    "                return next_state, penalty, False           #No goal reached, keep searching\n",
    "        else:\n",
    "            return state, penalty, False                    #Hit a wall, stay in the same state\n",
    "    else:\n",
    "        return state, penalty, False                        #Out of the maze, stay in the same state\n",
    "\n",
    "\n",
    "\n",
    "# Implementation of the starting funtion, in order to restart the agent's position in the maze\n",
    "# at the end of each episode.\n",
    "\n",
    "def restarting():\n",
    "    return start_position\n",
    "\n",
    "\n",
    "# Implementation of the selection of the agent's action.\n",
    "# The agent will randomly choose the actions to take in the Q-table, based on the epsilon-greedy policy.\n",
    "# The action with the highest Q-value will be chosen \n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return np.argmax(q_table[state[0], state[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Initial exploration rate. \n",
    "epsilon_variation = 0.99  # This parameter will decrease the epsilon value over time, in order to make the agent more greedy\n",
    "min_epsilon = 0.01  # Minimum exploration rate reached by the agent\n",
    "episodes = 1000 # Number of episodes the agent will run\n",
    "steps = 100  # Number of steps the agent will take in each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Q-learning algorithm based on the mathematical formula\n",
    "# Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))\n",
    "\n",
    "def q_value(state, action_index, reward, next_state):\n",
    "    \n",
    "    max_q_value = np.max(q_table[next_state[0], next_state[1]])\n",
    "    actual_q_value = q_table[state[0], state[1], action_index]\n",
    "    new_q_value = actual_q_value + alpha * (reward + gamma * max_q_value - actual_q_value)\n",
    "\n",
    "    q_table[state[0], state[1], action_index] = new_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000, Total reward: 307\n",
      "Episode 200/1000, Total reward: 373\n",
      "Episode 300/1000, Total reward: 417\n",
      "Episode 400/1000, Total reward: 417\n",
      "Episode 500/1000, Total reward: 428\n",
      "Episode 600/1000, Total reward: 417\n",
      "Episode 700/1000, Total reward: 428\n",
      "Episode 800/1000, Total reward: 428\n",
      "Episode 900/1000, Total reward: 428\n",
      "Episode 1000/1000, Total reward: 428\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    state = restarting()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    #if episode < 5:  # Print debug information for the first few episodes\n",
    "    #    print(f\"\\nStarting Episode {episode + 1}\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        action_index = select_action(state, epsilon)\n",
    "        action = actions[action_index]\n",
    "\n",
    "        # Debug: Print selected action and state\n",
    "    #    if episode < 5:\n",
    "    #        print(f\"Step {step + 1}: State: {state}, Action: {action}\")\n",
    "\n",
    "        next_state, reward, done = moves(state, action)\n",
    "\n",
    "        # Debug: Print next state, reward, and done flag\n",
    "    #    if episode < 5:\n",
    "    #        print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        # Update Q-value\n",
    "        max_future_q = np.max(q_table[next_state[0], next_state[1]])\n",
    "        current_q = q_table[state[0], state[1], action_index]\n",
    "        q_table[state[0], state[1], action_index] = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "    #        if episode < 5:\n",
    "    #            print(f\"Episode {episode + 1} completed after {step + 1} steps with total reward {total_reward}\")\n",
    "           break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_variation)\n",
    "\n",
    "    # Print progress every 100 episodes\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f'Episode {episode + 1}/{episodes}, Total reward: {total_reward}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
