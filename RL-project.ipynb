{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  *Develop an agent that can achieve a specific goal and sub-goal in a maze 10X10 based on RL algorithm (Q-learning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries to create the maze and give our agent the ability to randomly making choices\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemetation of the maze 10X10 as indicated in the assignment. \n",
    "\n",
    "# 1 represents the walls (unpassable)\n",
    "# 0 represents an open path (passable)\n",
    "# S marks the starting point of the agent\n",
    "# G marks the sub-goal the agent should reach before continuing\n",
    "# E marks the end goal\n",
    "\n",
    "maze = np.array([[1,1,1,1,1,1,1,1,1,1],\n",
    "                 [1,'S',1,0,0,0,1,0,0,1],\n",
    "                 [1,0,1,0,1,0,1,0,1,1],\n",
    "                 [1,0,0,0,1,0,0,0,0,1],\n",
    "                 [1,1,1,0,1,1,1,1,0,1],\n",
    "                 [1,0,1,'G',0,0,0,1,0,1],\n",
    "                 [1,0,0,0,1,1,0,0,0,1],\n",
    "                 [1,1,1,0,1,0,1,1,0,1],\n",
    "                 [1,0,0,0,0,0,1,'E',0,1],\n",
    "                 [1,1,1,1,1,1,1,1,1,1]], dtype=object)\n",
    "\n",
    "\n",
    "\n",
    "# Implementatio of the Q-Table\n",
    "# Considering the maze above, and the Q-learning algorithm, we need to initiate\n",
    "# the Q-table. The Q-table include all possible states-actions pairs and their corresponing Q-values.\n",
    "\n",
    "q_table = np.zeros((10,10,4))   # each states have 4 possible actions (up, down, left, right)\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Implementation of the agent's starting position in the maze, the sub-goal and the end-goal\n",
    "\n",
    "start_position = (1,1)\n",
    "sub_goal = (5,3)\n",
    "end_goal = (8,7)\n",
    "\n",
    "\n",
    "# Implementation of the reward system\n",
    "\n",
    "reward_sub_goal = 10\n",
    "reward_end_goal = 100\n",
    "penalty = -1    \n",
    "\n",
    "# Implementation of the agent's interactions with the environment, the way the agent moves in the maze\n",
    "# and the way penalties and rewards are given to the agent.\n",
    "\n",
    "def moves(state, actions):\n",
    "    row, col = state\n",
    "\n",
    "    if actions == 'up':\n",
    "        next_state = (row - 1, col)\n",
    "    elif actions == 'down':\n",
    "        next_state = (row + 1, col)\n",
    "    elif actions == 'left':\n",
    "        next_state = (row, col - 1)\n",
    "    elif actions == 'right':\n",
    "        next_state = (row, col + 1)\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "# Check if the next state is a wall or out of the maze\n",
    "    if next_state[0] >= 0 and next_state[0] < 10 and next_state[1] >= 0 and next_state[1] < 10:\n",
    "        if maze[next_state] != 1:\n",
    "            if next_state == (5,3):\n",
    "                return next_state, reward_sub_goal, False   #Sub-goal reached \n",
    "            elif next_state == (8,7):\n",
    "                return next_state, reward_end_goal, True    #End-goal reached\n",
    "            else:\n",
    "                return next_state, penalty, False           #No goal reached, keep searching\n",
    "        else:\n",
    "            return state, penalty, False                    #Hit a wall, stay in the same state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implementation of the starting funtion, in order to restart the agent's position in the maze\n",
    "# at the end of each episode.\n",
    "\n",
    "def restarting():\n",
    "    return start_position\n",
    "\n",
    "\n",
    "# Implementation of the selection of the agent's action.\n",
    "# The agent will randomly choose the actions to take in the Q-table, based on the epsilon-greedy policy.\n",
    "# The action with the highest Q-value will be chosen \n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return np.argmax(q_table[state[0], state[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "alpha = 0.01  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Initial exploration rate. \n",
    "epsislon_variation = 0.95  # This parameter will decrease the epsilon value over time, in order to make the agent more greedy\n",
    "min_epsilon = 0.01  # Minimum exploration rate reached by the agent\n",
    "episodes = 1000 # Number of episodes the agent will run\n",
    "steps = 1000  # Number of steps the agent will take in each episode\n",
    "\n",
    "# Implementation of the Q-learning algorithm based on the mathematical formula\n",
    "# Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))\n",
    "\n",
    "def q_value(state, action_index, reward, next_state):\n",
    "    \n",
    "    max_q_value = np.max(q_table[next_state[0], next_state[1]])\n",
    "    actual_q_value = q_table[state[0], state[1], action_index]\n",
    "    new_q_value = actual_q_value + alpha * (reward + gamma * max_q_value - actual_q_value)\n",
    "\n",
    "    q_table[state[0], state[1], action_index] = new_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/99, Total reward: -901\n",
      "Episode 200/199, Total reward: -801\n",
      "Episode 300/299, Total reward: -701\n",
      "Episode 400/399, Total reward: -601\n",
      "Episode 500/499, Total reward: -501\n",
      "Episode 600/599, Total reward: -401\n",
      "Episode 700/699, Total reward: -301\n",
      "Episode 800/799, Total reward: -201\n",
      "Episode 900/899, Total reward: -101\n",
      "Episode 1000/999, Total reward: -1\n"
     ]
    }
   ],
   "source": [
    "for episodes in range(episodes):\n",
    "    \n",
    "    state = restarting()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for steps in range(steps):\n",
    "\n",
    "        actions_index = select_action(state, epsilon)\n",
    "        action = actions[actions_index]\n",
    "\n",
    "        next_state, reward, done = moves(state, action)\n",
    "\n",
    "        q_value(state, actions_index, reward, next_state)\n",
    "\n",
    "        state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon = max(min_epsilon, epsilon * epsislon_variation)\n",
    "\n",
    "    if (episodes + 1) % 100 == 0:\n",
    "        print(f'Episode {episodes + 1}/{episodes}, Total reward: {total_reward}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
